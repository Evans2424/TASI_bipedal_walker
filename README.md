# Bipedal Walker Reinforcement Learning Project

A comprehensive implementation of state-of-the-art reinforcement learning algorithms for training agents to solve the OpenAI Gymnasium Bipedal Walker environment.

## ðŸš€ GPU-Accelerated Training (NEW!)

**For L40S GPU users**: Get **8-10x faster training** with GPU optimization!

```bash
# Fastest option: Stable-Baselines3 + GPU (45 min vs 8 hours)
python train_sb3_gpu.py --config configs/sac_sb3_gpu.yaml
```

ðŸ“– **Quick References:**
- `GPU_SUMMARY.md` - Quick start guide for GPU training
- `GPU_TRAINING_GUIDE.md` - Complete GPU optimization guide  
- `SB3_VS_CUSTOM.md` - Comparison of implementations

---

## Table of Contents

- [Overview](#overview)
- [Environment Details](#environment-details)
- [Project Structure](#project-structure)
- [Implemented Algorithms](#implemented-algorithms)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Training](#training)
- [Evaluation](#evaluation)
- [Architecture Details](#architecture-details)
- [Configuration](#configuration)
- [Results Visualization](#results-visualization)
- [GPU Optimization](#gpu-optimization)

## Overview

This project implements a production-ready reinforcement learning training framework for the Bipedal Walker environment. The walker is a 4-jointed robot that must learn to walk across terrain using continuous control.

**Key Features:**
- Multiple RL algorithms (PPO, SAC) with modular architecture
- GPU-accelerated training optimized for L40 GPUs
- Comprehensive logging with TensorBoard
- Model checkpointing and evaluation tools
- Video recording capabilities
- Configurable hyperparameters via YAML files
- Clean, maintainable code following best practices

## Environment Details

### Bipedal Walker (Normal Mode)
- **Observation Space**: 24-dimensional continuous vector
  - Hull angle, angular velocity
  - Horizontal and vertical speeds
  - Joint positions and angular speeds
  - Leg-ground contact information
  - 10 LIDAR rangefinder measurements
- **Action Space**: 4-dimensional continuous vector in [-1, 1]
  - Motor speeds for hip and knee joints
- **Reward Structure**:
  - Forward progress: positive rewards
  - Falling: -100 penalty
  - Motor torque: small negative cost
- **Success Criteria**: 300+ points within 1600 timesteps

### Bipedal Walker Hardcore Mode
- Same as normal mode with additional challenges:
  - Ladders, stumps, and pitfalls
  - 2000 timesteps allowed
  - Significantly more difficult

## Project Structure

```
bipedal_walker/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ ppo_config.yaml        # PPO hyperparameters
â”‚   â””â”€â”€ sac_config.yaml        # SAC hyperparameters
â”‚
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ agents/               # RL agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py    # Abstract base class
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py     # Proximal Policy Optimization
â”‚   â”‚   â””â”€â”€ sac_agent.py     # Soft Actor-Critic
â”‚   â”‚
â”‚   â”œâ”€â”€ models/               # Neural network architectures
â”‚   â”‚   â””â”€â”€ networks.py      # Actor, Critic, Value networks
â”‚   â”‚
â”‚   â”œâ”€â”€ envs/                # Environment wrappers
â”‚   â”‚   â””â”€â”€ env_wrapper.py   # Custom environment utilities
â”‚   â”‚
â”‚   â””â”€â”€ utils/               # Utility modules
â”‚       â”œâ”€â”€ replay_buffer.py # Experience replay buffers
â”‚       â”œâ”€â”€ logger.py        # TensorBoard logging
â”‚       â””â”€â”€ seed.py          # Reproducibility utilities
â”‚
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ evaluate.py          # Model evaluation
â”‚   â”œâ”€â”€ record_video.py      # Video recording
â”‚   â””â”€â”€ plot_results.py      # Results visualization
â”‚
â”œâ”€â”€ experiments/             # Training artifacts (gitignored)
â”‚   â”œâ”€â”€ checkpoints/        # Model checkpoints
â”‚   â”œâ”€â”€ logs/              # TensorBoard logs
â”‚   â””â”€â”€ videos/            # Recorded videos
â”‚
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ notebooks/             # Jupyter notebooks for analysis
â”œâ”€â”€ data/                  # Optional: saved trajectories
â”œâ”€â”€ train.py              # Main training script
â””â”€â”€ requirements.txt      # Python dependencies
```

## Implemented Algorithms

### 1. Proximal Policy Optimization (PPO)

**PPO** is an on-policy algorithm that uses a clipped surrogate objective to prevent large policy updates, ensuring stable training.

**Key Components:**
- **Actor Network**: Gaussian policy (mean and std) for continuous actions
- **Critic Network**: State value function V(s)
- **Advantage Estimation**: Generalized Advantage Estimation (GAE)
- **Update Strategy**: Multiple epochs of mini-batch updates on collected rollouts

**Implementation Details** (`src/agents/ppo_agent.py`):
```python
# Key hyperparameters
learning_rate: 3e-4
gamma: 0.99              # Discount factor
gae_lambda: 0.95         # GAE parameter
clip_epsilon: 0.2        # PPO clipping parameter
value_loss_coef: 0.5     # Value loss coefficient
entropy_coef: 0.01       # Entropy bonus
ppo_epochs: 10           # Update epochs per rollout
mini_batch_size: 64      # Mini-batch size
rollout_steps: 2048      # Steps before update
```

**Training Flow:**
1. Collect rollout of experiences (2048 steps)
2. Compute advantages using GAE
3. Perform multiple epochs of mini-batch updates
4. Clip policy ratio to prevent large updates
5. Optimize policy and value function jointly

### 2. Soft Actor-Critic (SAC)

**SAC** is an off-policy algorithm that maximizes both expected return and entropy, encouraging exploration while learning.

**Key Components:**
- **Actor Network**: Stochastic Gaussian policy with reparameterization trick
- **Twin Critics**: Two Q-networks to mitigate overestimation bias
- **Target Networks**: Soft-updated target critics for stability
- **Automatic Entropy Tuning**: Adaptive temperature parameter Î±

**Implementation Details** (`src/agents/sac_agent.py`):
```python
# Key hyperparameters
learning_rate: 3e-4
gamma: 0.99                      # Discount factor
tau: 0.005                       # Soft update coefficient
alpha: 0.2                       # Initial entropy coefficient
automatic_entropy_tuning: true   # Auto-tune Î±
buffer_capacity: 1000000         # Replay buffer size
batch_size: 256                  # Training batch size
learning_starts: 10000           # Random exploration steps
```

**Training Flow:**
1. Collect experience with stochastic policy
2. Store transitions in replay buffer
3. Sample random mini-batches
4. Update twin critics using Bellman equation
5. Update actor to maximize Q-value and entropy
6. Soft-update target networks
7. Optionally tune entropy coefficient Î±

## Installation

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (optional, recommended for training)
- pip or conda

### Setup

1. **Clone or navigate to the repository:**
```bash
cd /path/to/bipedal_walker
```

2. **Create virtual environment (recommended):**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Install Box2D (if issues occur):**
```bash
# On macOS
brew install swig
pip install box2d-py

# On Ubuntu
sudo apt-get install swig
pip install box2d-py
```

## Quick Start

### Train PPO agent:
```bash
python train.py --config configs/ppo_config.yaml
```

### Train SAC agent:
```bash
python train.py --config configs/sac_config.yaml
```

### Evaluate trained agent:
```bash
python scripts/evaluate.py \
    --checkpoint experiments/checkpoints/ppo_bipedal_walker/final_model.pt \
    --config configs/ppo_config.yaml \
    --episodes 10 \
    --render
```

### Record videos:
```bash
python scripts/record_video.py \
    --checkpoint experiments/checkpoints/ppo_bipedal_walker/final_model.pt \
    --config configs/ppo_config.yaml \
    --episodes 3
```

## Training

### Training Process

The training script (`train.py`) follows this workflow:

1. **Environment Setup**: Creates Gymnasium environment with custom wrappers
2. **Agent Initialization**: Instantiates selected algorithm (PPO/SAC)
3. **Buffer Creation**: Sets up replay buffer (SAC) or rollout buffer (PPO)
4. **Logger Setup**: Initializes TensorBoard logging
5. **Training Loop**:
   - Collect experiences by interacting with environment
   - Update agent parameters
   - Evaluate periodically
   - Save checkpoints
   - Log metrics

### Monitoring Training

View real-time training metrics with TensorBoard:
```bash
tensorboard --logdir experiments/logs
```

Navigate to `http://localhost:6006` to view:
- Episode rewards and lengths
- Training losses (actor, critic, value)
- Policy entropy
- Learning rate schedules
- Evaluation performance

### Training Tips

**For PPO:**
- Start with default hyperparameters
- If training is unstable, reduce `learning_rate` or `clip_epsilon`
- If exploration is insufficient, increase `entropy_coef`
- Typical training time: 1-2M timesteps (~2-4 hours on L40 GPU)

**For SAC:**
- SAC is more sample-efficient but slower per step
- Let automatic entropy tuning adapt Î±
- If too conservative, decrease initial `alpha`
- Typical training time: 500K-1M timesteps (~1-2 hours on L40 GPU)

**Hardware Recommendations:**
- L40 GPU: Excellent for this task
- Batch size can be increased for better GPU utilization
- Consider parallel environments for faster data collection

## Evaluation

### Standard Evaluation

Evaluate on normal mode:
```bash
python scripts/evaluate.py \
    --checkpoint experiments/checkpoints/ppo_bipedal_walker/final_model.pt \
    --config configs/ppo_config.yaml \
    --episodes 10
```

### Hardcore Evaluation

Test on hardcore mode:
```bash
python scripts/evaluate.py \
    --checkpoint experiments/checkpoints/ppo_bipedal_walker/final_model.pt \
    --config configs/ppo_config.yaml \
    --episodes 10 \
    --hardcore
```

### Visualization

Plot training curves:
```bash
python scripts/plot_results.py \
    --log-dir experiments/logs/ppo_bipedal_walker \
    --output training_plot.png
```

## Architecture Details

### Neural Network Architecture

All networks use multi-layer perceptrons (MLPs) with the following structure:

**Default Architecture:**
```
Input Layer â†’ Linear(obs_dim, 256) â†’ ReLU
           â†’ Linear(256, 256) â†’ ReLU
           â†’ Linear(256, output_dim)
```

**Implemented Networks** (`src/models/networks.py`):

1. **GaussianActor**: Stochastic policy for PPO/SAC
   - Outputs: mean and log_std for Gaussian distribution
   - Supports reparameterization trick
   - Tanh squashing for bounded actions

2. **Critic**: Q-function for SAC
   - Input: concatenated (state, action)
   - Output: single Q-value

3. **StateValueNetwork**: V-function for PPO
   - Input: state
   - Output: state value

4. **Actor** (deterministic): For DDPG/TD3 (included for future extensions)

### Agent Class Hierarchy

```
BaseAgent (abstract)
â”œâ”€â”€ PPOAgent
â””â”€â”€ SACAgent
```

**BaseAgent** provides:
- Common interface for all algorithms
- Reproducibility (seed management)
- Device management (CPU/GPU)
- Save/load functionality

**Algorithm-Specific Agents** implement:
- `select_action()`: Action selection logic
- `update()`: Parameter update logic
- Algorithm-specific components

## Configuration

Configuration files use YAML format for easy modification.

### Key Configuration Sections

**Environment Settings:**
```yaml
env:
  name: "BipedalWalker-v3"
  hardcore: false
  reward_scale: 1.0
  clip_observations: false
  clip_actions: true
```

**Agent Hyperparameters:**
```yaml
agent:
  type: "ppo"  # or "sac"
  hidden_dims: [256, 256]
  learning_rate: 3.0e-4
  gamma: 0.99
  # ... algorithm-specific parameters
```

**Training Settings:**
```yaml
training:
  total_timesteps: 2000000
  eval_frequency: 10000
  save_frequency: 50000
  log_frequency: 1000
```

**Experiment Tracking:**
```yaml
experiment:
  name: "ppo_bipedal_walker"
  seed: 42
  device: "cuda"
```

### Creating Custom Configurations

1. Copy existing config file
2. Modify hyperparameters
3. Update experiment name
4. Run with: `python train.py --config path/to/config.yaml`

## Results Visualization

### TensorBoard

Real-time monitoring:
```bash
tensorboard --logdir experiments/logs
```

**Available Metrics:**
- `episode/reward`: Raw episode rewards
- `episode/mean_reward_100`: Rolling mean (last 100 episodes)
- `episode/length`: Episode lengths
- `train/policy_loss`: Policy loss (PPO) or actor loss (SAC)
- `train/value_loss`: Value function loss
- `train/entropy`: Policy entropy
- `eval/mean_reward`: Evaluation performance

### Plotting Scripts

Generate publication-quality plots:
```bash
python scripts/plot_results.py \
    --log-dir experiments/logs/ppo_bipedal_walker \
    --output results.png
```

### Video Analysis

Record agent behavior:
```bash
python scripts/record_video.py \
    --checkpoint experiments/checkpoints/ppo_bipedal_walker/checkpoint_1000000.pt \
    --config configs/ppo_config.yaml \
    --episodes 5
```

Videos saved to `experiments/videos/` in MP4 format.

## GPU Optimization

### L40 GPU Specific Optimizations

The L40 GPU has 48GB VRAM and excellent FP32/FP16 performance. Optimizations included:

**1. Batch Size Scaling**
- Default batch sizes are conservative
- Can increase PPO mini_batch_size to 128-256
- Can increase SAC batch_size to 512-1024
- Monitor GPU utilization with `nvidia-smi`

**2. Parallel Environments**
- Currently single environment
- Can implement vectorized environments for data collection speedup
- Modify `num_envs` in config (implementation required)

**3. Mixed Precision Training**
- PyTorch AMP can be enabled for 2x speedup
- Add to training loop:
```python
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
```

**4. Model Size**
- Default: [256, 256] hidden layers
- Can increase to [512, 512] or [1024, 1024]
- Larger models may improve performance on complex tasks

**5. Replay Buffer Optimization**
- Keep replay buffer on CPU to save GPU memory
- Transfer batches to GPU only during training
- Already implemented in current design

### Performance Benchmarks

Expected performance on L40 GPU:

| Algorithm | Steps/sec | Training Time (1M steps) | GPU Utilization |
|-----------|-----------|--------------------------|-----------------|
| PPO       | ~5000     | ~3-4 minutes            | ~30-40%         |
| SAC       | ~3000     | ~5-6 minutes            | ~40-50%         |

Note: Bottleneck is typically environment simulation, not GPU compute.

## Expected Training Results

### Performance Targets

**Normal Mode:**
- **PPO**: Should achieve 300+ reward within 1-2M timesteps
- **SAC**: Should achieve 300+ reward within 500K-1M timesteps

**Hardcore Mode:**
- Significantly more challenging
- May require 3-5M timesteps
- Consider transfer learning from normal mode

### Troubleshooting

**Low rewards (<100):**
- Check learning rate (try 1e-4 to 3e-4)
- Verify environment rendering works
- Ensure actions are not clipped incorrectly

**Training instability:**
- Reduce learning rate
- Increase batch size
- Adjust clipping parameters (PPO)
- Check gradient norms in logs

**Slow convergence:**
- Increase entropy coefficient (more exploration)
- Try different random seeds
- Verify GPU is being utilized

## Advanced Features

### Custom Reward Shaping

Modify reward in `src/envs/env_wrapper.py`:
```python
def step(self, action):
    # ... existing code ...
    reward = custom_reward_function(observation, reward, info)
    return observation, reward, terminated, truncated, info
```

### Curriculum Learning

Train on normal mode first, then fine-tune on hardcore:
```bash
# Train on normal
python train.py --config configs/ppo_config.yaml

# Fine-tune on hardcore
python train.py --config configs/ppo_hardcore_config.yaml \
    --load experiments/checkpoints/ppo_bipedal_walker/final_model.pt
```

### Hyperparameter Tuning

Use Weights & Biases (W&B) for sweeps:
1. Uncomment `wandb` in requirements.txt
2. Initialize wandb in train.py
3. Create sweep configuration
4. Run hyperparameter search

## Contributing

This project follows clean code principles:
- Modular design with clear separation of concerns
- Type hints for better code clarity
- Comprehensive docstrings
- Unit tests for critical components

To add new algorithms:
1. Create new agent class inheriting from `BaseAgent`
2. Implement required methods
3. Add configuration file
4. Update `train.py` to support new agent

## License

This project is for educational purposes.

## References

1. Schulman et al. (2017) - Proximal Policy Optimization Algorithms
2. Haarnoja et al. (2018) - Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL
3. OpenAI Gymnasium Documentation: https://gymnasium.farama.org/
4. Bipedal Walker Environment: https://gymnasium.farama.org/environments/box2d/bipedal_walker/

## Acknowledgments

- OpenAI Gymnasium for the environment
- PyTorch team for the deep learning framework
- The RL research community for algorithmic innovations

---

**Good luck with your training!** ðŸ¦¾ðŸ¤–
